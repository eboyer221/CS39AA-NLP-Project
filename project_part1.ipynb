{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 1 - RateMyProfessor Analysis\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction/Background\n",
    "\n",
    "The majority, if not all, educational institutions employ student feedback as a means of evaluating their professors. This feedback typically comprises both ratings and open-ended comments from students. However, in most cases this information is kept confidential and isn't disclosed to either current or prospective students. RateMyProfessors.com (RMP) is one popular, publicly-available platform where students anonymously rate various facets of their instructors' performance, such as clarity, helpfulness, and easiness. Additionally, they can provide open-ended comments.\n",
    "\n",
    "I would like to use this data to better understand what qualitative factors contribute to a professor's rating on RMP. The first goal is to determine whether automatic text classification can distinguish between professors with a high rating (>= 3.5) vs. professors with an average to low rating (< 3.5). I also want to examine student comments to isolate which factors are valued most by students, and whether differences emerge in those factors depending on the department the professor teaches in, as well as the gender of the professor.\n",
    "\n",
    "The dataset used for this project is taken from https://data.mendeley.com/datasets/fvtfjyvw7d/2 and includes ratings and comments for 4406 unique professors. The data set scraped from RMP includes 18 variables. The variable that I am primarily focused on predicting is the star rating of the professor's overall quality, which each student provides along with their review, and which is also combined into an average overall score. According to RMP’s official standard, a rating of 3.5-5.0 is good, 2.5-3.4 is average and 1.0-2.4 is poor. The goal is to develop a model that can automatically analyze and understand the sentiment and content of the comments provided by each student in order to predict the corresponding star rating.\n",
    "\n",
    "Dataset Citation:\n",
    "He, Jibo (2020), “Big Data Set from RateMyProfessor.com for Professors' Teaching Evaluation”, Mendeley Data, V2, doi: 10.17632/fvtfjyvw7d.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "The original data contains many single .csv files, with each file for a single professor's webpage from RMP. This requires some pre-processing. \n",
    "\n",
    "First, the individual .csv files need to be combined into one merged_data.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2 in /home/smoo/.local/lib/python3.10/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/smoo/.local/lib/python3.10/site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/smoo/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/smoo/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the python modules/packages you'll need here\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#code used on local machine to create the merged data set\\nfolder_path = 'folder_path_name'\\nfile_list = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\\n\\n# Initialize an empty DataFrame to store the merged data\\nmerged_data = pd.DataFrame()\\n\\n# Iterate through each file and concatenate it to the merged_data DataFrame\\nfor file in file_list:\\n    file_path = os.path.join(folder_path, file)\\n    df = pd.read_csv(file_path)\\n    merged_data = pd.concat([merged_data, df], ignore_index=True)\\n#save the merged data to a new CSV file\\n    merged_data.to_csv('merged_data.csv', index=False)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#code used on local machine to create the merged data set\n",
    "folder_path = 'folder_path_name'\n",
    "file_list = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file and concatenate it to the merged_data DataFrame\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "#save the merged data to a new CSV file\n",
    "    merged_data.to_csv('merged_data.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the merged data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3808/2226498928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/eboyer221/CS39AA-Project/blob/main/merged_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_URL = 'https://github.com/eboyer221/CS39AA-Project/blob/main/merged_data.csv'\n",
    "df = pd.read_csv(data_URL)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
